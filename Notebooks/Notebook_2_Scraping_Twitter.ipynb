{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Naomiweinberger/Hate-Speech/blob/main/scraping_twitter_pynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scraping Twitter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**WARNING: DUE TO THE NATURE OF THE PROJECT,THIS NOTEBOOK CONTAINS HATEFUL SPEECH,THIS IN NO WAY REFLECTS THE VIEWS OF THE AUTHOR.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, I used key words from each class to search new tweets. I combined those tweets together to form a validation set. Check out [Notebook 3](http://localhost:8889/notebooks/Notebooks/Notebook_3_Data_Preprocessing_NLTK_and_Models.ipynb) and [Notebook 4](http://localhost:8889/notebooks/Notebooks/Notebook_4_Data_Preprocessing_Spacy_and_models.ipynb) to see how I tested my models on this set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I imported the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OL9FTCfxJFYl"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, csv, logging, urllib, urllib3, json,bson, re,string\n",
    "from bson import json_util\n",
    "from datetime import datetime, timedelta\n",
    "from datetime import time\n",
    "import collections ,sys,math, time, datetime\n",
    "import pickle\n",
    "from config import CONSUMER_TOKEN,CONSUMER_SECRET,access_token,access_token_secret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then used my twitter developers API codes that I recieved from them. I used tweepy to search for tweets using the key words. I saved them to a csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sLRnCMxeM18Y"
   },
   "outputs": [],
   "source": [
    "auth  = tweepy.OAuthHandler(CONSUMER_TOKEN, \\\n",
    "                            CONSUMER_SECRET)\n",
    "auth.set_access_token(access_token,  \\\n",
    "                      access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each class, I searched for words that appeared significant during my EDA. For more information check out [Notebook#1](http://localhost:8889/notebooks/Notebooks/Notebook_1_EDA.ipynb). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I turned each search into a dataframe and then concated the dataframes together and saved it as a csv file. I then used that csv file as a validation set for my models. See [Notebook3](http://localhost:8889/notebooks/Notebooks/Notebook_3_Data_Preprocessing_NLTK_and_Models.ipynb) and [Notebook4](http://localhost:8889/notebooks/Notebooks/Notebook_4_Data_Preprocessing_Spacy_and_models.ipynb) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xi1YkBkH__Qt"
   },
   "source": [
    "**Offensive Tweets** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "seTX7reMm7TL"
   },
   "outputs": [],
   "source": [
    "o_tweets = api.search('pussy', count=100)\n",
    "o_tweet=[]\n",
    "for tweet in o_tweets:\n",
    "  o_tweet.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKY4FTFDnTFt"
   },
   "outputs": [],
   "source": [
    "df_o=pd.DataFrame(o_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HWRlvPCCn2Vb"
   },
   "outputs": [],
   "source": [
    "df_o['tweet']=df_o\n",
    "df_o['label']=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqDU45LUn_VE"
   },
   "outputs": [],
   "source": [
    "df_o=df_o[['tweet','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hate Tweets** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ivXzHTMnoAGk"
   },
   "outputs": [],
   "source": [
    "h_tweets = api.search('faggot', count=100)\n",
    "h_tweet=[]\n",
    "for tweet in h_tweets:\n",
    "  h_tweet.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DD95_ZyaoQNc"
   },
   "outputs": [],
   "source": [
    "df_h=pd.DataFrame(h_tweet)\n",
    "df_h['tweet']=df_h\n",
    "df_h['label']=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8wKaP_Gpzzz"
   },
   "outputs": [],
   "source": [
    "df_h=df_h[['tweet','label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normal Tweets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R0cdDSe2ojz0"
   },
   "outputs": [],
   "source": [
    "n_tweets = api.search('happy', count=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5W5L6repIXc"
   },
   "outputs": [],
   "source": [
    "n_tweet=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "REHCjcWfpRkc"
   },
   "outputs": [],
   "source": [
    "for tweet in n_tweets:\n",
    "  n_tweet.append(tweet.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fedLnIhPpbkN"
   },
   "outputs": [],
   "source": [
    "df_n=pd.DataFrame(n_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxoksyjhpeFM"
   },
   "outputs": [],
   "source": [
    "df_n['tweet']=df_n\n",
    "df_n['label']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VtGlwT9Op4Qu"
   },
   "outputs": [],
   "source": [
    "df_n=df_n[['tweet','label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbMJdkbXpfNU"
   },
   "outputs": [],
   "source": [
    "df_take2=pd.concat([df_h,df_o,df_n])\n",
    "df_take2.to_csv('twitter_3.csv') "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOliTQ621uL22jIZpiGfWn/",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "173UBmsd4icOiw-4fk5U1VuvZWSzL0ZTF",
   "name": "scraping_twitter.pynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
